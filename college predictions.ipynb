{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "conn = sqlite3.connect(\"profiles.db\")\n",
    "\n",
    "def convert_class_rank(cr_str):\n",
    "    cr = cr_str.split(' ')\n",
    "    return int(cr[0])/int(cr[2])\n",
    "\n",
    "def get_type_map(df_column):\n",
    "    return {t:i for i, t in enumerate(set(df_column))}\n",
    "\n",
    "def convert_sat_to_act(sat):\n",
    "    if sat >= 2380:\n",
    "        return 36\n",
    "    if sat < 2380 and sat >= 2290:\n",
    "        return 35\n",
    "    if sat < 2290 and sat >= 2220:\n",
    "        return 34\n",
    "    if sat < 2220 and sat >= 2140:\n",
    "        return 33\n",
    "    if sat < 2140 and sat >= 2080:\n",
    "        return 32\n",
    "    if sat < 2080 and sat >= 2020:\n",
    "        return 31\n",
    "    if sat < 2020 and sat >= 1980:\n",
    "        return 30\n",
    "    if sat < 1980 and sat >= 1920:\n",
    "        return 29\n",
    "    if sat < 1920 and sat >= 1860:\n",
    "        return 28\n",
    "    if sat < 1860 and sat >= 1800:\n",
    "        return 27\n",
    "    if sat < 1800 and sat >= 1740:\n",
    "        return 26\n",
    "    if sat < 1740 and sat >= 1680:\n",
    "        return 25\n",
    "    if sat < 1680 and sat >= 1620:\n",
    "        return 24\n",
    "    if sat < 1620 and sat >= 1560:\n",
    "        return 23\n",
    "    if sat < 1560 and sat >= 1510:\n",
    "        return 22\n",
    "    if sat < 1510 and sat >= 1450:\n",
    "        return 21\n",
    "    if sat < 1450 and sat >= 1390:\n",
    "        return 20\n",
    "    if sat < 1390 and sat >= 1330:\n",
    "        return 19\n",
    "    if sat < 1330 and sat >= 1270:\n",
    "        return 18\n",
    "    if sat < 1270 and sat >= 1210:\n",
    "        return 17\n",
    "    if sat < 1210 and sat >= 1140:\n",
    "        return 16\n",
    "    if sat < 1140 and sat >= 1060:\n",
    "        return 15\n",
    "    if sat < 1060 and sat >= 990:\n",
    "        return 14\n",
    "    if sat < 990:\n",
    "        return 13\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM profiles\", conn)\n",
    "# sums individual scores and drops them\n",
    "df['sat_c'] = df['sat_m']+df['sat_r']+df['sat_w']\n",
    "for col in ['sat_m', 'sat_r', 'sat_w']:\n",
    "    df = df.drop(col, axis=1)\n",
    "# converts all sat to act if act is not present\n",
    "df['act'] = df.apply(lambda x: convert_sat_to_act(x['sat_c'] if pd.isnull(x['act']) else x['act']), axis=1)\n",
    "# drop nan\n",
    "df = df.dropna()\n",
    "# drop class ranks that are not in \"1 of 200\" format\n",
    "df = df[df.class_rank.str.match('\\d* (of) \\d*', na=False)]\n",
    "# converts class rank to decimal\n",
    "df['class_rank'] = df['class_rank'].apply(lambda x: convert_class_rank(x))\n",
    "# convert string label to int\n",
    "replace_columns = ['hs_type', 'gender', 'status', 'hs_state', 'school']\n",
    "df['status'].replace({'Deferred': 'Denied', 'Wait-Listed': 'Denied', 'Will Attend': 'Accepted'}, inplace=True)\n",
    "for col in replace_columns:\n",
    "    df[col].replace(get_type_map(df[col]), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       school  gender  gpa_w  gpa_uw   act  class_rank  eaed  legacy\n",
      "0          47       2   3.50    3.00  20.0    0.210753     0       0\n",
      "3          49       2   4.50    3.90  13.0    0.017544     0       0\n",
      "7          49       2   4.00    3.80  25.0    0.075231     0       0\n",
      "9          49       2   3.90    3.60  31.0    0.149626     0       0\n",
      "11         49       2   3.80    3.60  25.0    0.097059     0       0\n",
      "15         49       2   4.00    4.00  32.0    0.009259     0       0\n",
      "17         49       2   3.89    3.47  24.0    0.145251     1       0\n",
      "19         49       2   4.00    3.80  13.0    0.087912     0       0\n",
      "21         49       2   3.90    3.90  26.0    0.055556     1       0\n",
      "22         49       0   4.00    3.70  13.0    0.079295     0       0\n",
      "23         49       2   3.06    2.65  24.0    0.093023     1       0\n",
      "24         49       0   3.30    3.20  26.0    0.383966     1       0\n",
      "26         49       0   3.70    3.60  26.0    0.164948     0       0\n",
      "30         49       0   3.85    3.75  26.0    0.131012     0       0\n",
      "34         49       2   3.80    3.70  24.0    0.100000     0       0\n",
      "47         49       2   3.90    3.80  13.0    0.117647     0       0\n",
      "52         49       2   3.80    3.60  23.0    0.032086     0       0\n",
      "53         49       2   4.14    3.67  27.0    0.084956     0       0\n",
      "55         49       2   3.80    3.50  27.0    0.330000     0       0\n",
      "56         49       2   3.70    3.70  23.0    0.321739     0       0\n",
      "57         49       2   4.00    4.00  29.0    0.016568     0       0\n",
      "59         49       2   4.00    3.90  13.0    0.054545     0       0\n",
      "63         49       2   3.80    3.40  28.0    0.311111     1       0\n",
      "65         49       0   3.40    3.20  25.0    0.435115     1       0\n",
      "70         49       2   3.80    3.80  25.0    0.785714     1       0\n",
      "73         49       2   4.00    3.30  13.0    0.363934     0       0\n",
      "78         52       2   4.00    3.80  25.0    0.075231     0       0\n",
      "82         52       2   5.00    3.40  24.0    0.035556     1       0\n",
      "83         52       2   2.77    2.60  32.0    0.369458     0       0\n",
      "102        43       2   5.00    4.00  23.0    0.094595     0       0\n",
      "...       ...     ...    ...     ...   ...         ...   ...     ...\n",
      "13042      82       0   4.67    4.00  13.0    0.007380     1       0\n",
      "13044      82       0   4.62    3.99  13.0    0.024390     1       0\n",
      "13045      82       2   3.94    3.45  30.0    0.434251     0       0\n",
      "13050      82       0   4.20    4.00  13.0    0.078526     0       0\n",
      "13059      82       2   3.98    3.98  28.0    0.025000     0       0\n",
      "13062      82       0   4.70    3.95  13.0    0.022581     0       0\n",
      "13065      82       2   4.00    4.00  13.0    0.012195     0       0\n",
      "13067      82       2   4.05    3.60  13.0    0.165000     1       0\n",
      "13068      82       2   4.30    3.70  13.0    0.041344     0       0\n",
      "13070      82       2   4.04    3.77  13.0    0.068493     1       0\n",
      "13085      82       2   4.67    3.88  28.0    0.048828     0       0\n",
      "13087      82       0   4.40    3.80  13.0    0.083333     1       0\n",
      "13096      82       2   4.42    3.94  13.0    0.020408     1       0\n",
      "13101      82       2   4.30    4.00  13.0    0.037500     0       0\n",
      "13102      82       0   4.50    3.91  13.0    0.013333     0       0\n",
      "13110      82       2   4.89    3.56  13.0    0.019048     1       0\n",
      "13118      82       2   4.65    4.00  31.0    0.060870     1       0\n",
      "13119      82       2   4.13    3.86  13.0    0.015385     0       0\n",
      "13122      82       0   4.60    3.83  13.0    0.008108     1       0\n",
      "13128      82       2   4.10    4.00  29.0    0.023622     1       0\n",
      "13131      82       0   4.00    3.70  32.0    0.060000     1       0\n",
      "13132      82       0   4.50    3.60  31.0    0.037109     0       0\n",
      "13133      82       0   4.30    3.90  13.0    0.012121     0       1\n",
      "13135      82       0   4.13    3.97  13.0    0.073077     0       0\n",
      "13139      82       2   3.73    3.53  27.0    0.121212     0       0\n",
      "13142      82       0   3.81    3.62  33.0    0.245455     0       0\n",
      "13143      82       0   5.00    4.00  13.0    0.002000     1       0\n",
      "13151      82       0   4.01    3.78  13.0    0.047619     0       0\n",
      "13154      82       0   4.21    3.85  13.0    0.013158     1       0\n",
      "13160      82       2   4.10    3.70  32.0    0.030612     0       0\n",
      "\n",
      "[4101 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('year', axis=1)\n",
    "X = X.drop('status', axis=1)\n",
    "X = X.drop('hs_type', axis=1)\n",
    "X = X.drop('hs_state', axis=1)\n",
    "X = X.drop('athlete', axis=1)\n",
    "X = X.drop('sat_c', axis=1)\n",
    "print(X)\n",
    "y = df['status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\peaka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\users\\peaka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\peaka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,8,8),max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(8, 8, 8), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 72 209]\n",
      " [ 65 680]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.26      0.34       281\n",
      "           1       0.76      0.91      0.83       745\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1026\n",
      "   macro avg       0.65      0.58      0.59      1026\n",
      "weighted avg       0.70      0.73      0.70      1026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7261208576998051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\peaka\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
