{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "conn = sqlite3.connect(\"profiles.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"ATTACH DATABASE 'colleges.db' AS colleges\")\n",
    "\n",
    "# print(cursor.execute(\"\"\"\n",
    "# SELECT * from colleges.university_profiles as c JOIN profiles as p on c.school=p.school limit 2\n",
    "# \"\"\").fetchall())\n",
    "\n",
    "def convert_class_rank(cr_str):\n",
    "    cr = cr_str.split(' ')\n",
    "    return int(cr[0])/int(cr[2])\n",
    "\n",
    "def get_type_map(df_column):\n",
    "    return {t:i for i, t in enumerate(set(df_column))}\n",
    "\n",
    "def convert_sat_to_act(sat):\n",
    "    if sat >= 2380:\n",
    "        return 36\n",
    "    if sat < 2380 and sat >= 2290:\n",
    "        return 35\n",
    "    if sat < 2290 and sat >= 2220:\n",
    "        return 34\n",
    "    if sat < 2220 and sat >= 2140:\n",
    "        return 33\n",
    "    if sat < 2140 and sat >= 2080:\n",
    "        return 32\n",
    "    if sat < 2080 and sat >= 2020:\n",
    "        return 31\n",
    "    if sat < 2020 and sat >= 1980:\n",
    "        return 30\n",
    "    if sat < 1980 and sat >= 1920:\n",
    "        return 29\n",
    "    if sat < 1920 and sat >= 1860:\n",
    "        return 28\n",
    "    if sat < 1860 and sat >= 1800:\n",
    "        return 27\n",
    "    if sat < 1800 and sat >= 1740:\n",
    "        return 26\n",
    "    if sat < 1740 and sat >= 1680:\n",
    "        return 25\n",
    "    if sat < 1680 and sat >= 1620:\n",
    "        return 24\n",
    "    if sat < 1620 and sat >= 1560:\n",
    "        return 23\n",
    "    if sat < 1560 and sat >= 1510:\n",
    "        return 22\n",
    "    if sat < 1510 and sat >= 1450:\n",
    "        return 21\n",
    "    if sat < 1450 and sat >= 1390:\n",
    "        return 20\n",
    "    if sat < 1390 and sat >= 1330:\n",
    "        return 19\n",
    "    if sat < 1330 and sat >= 1270:\n",
    "        return 18\n",
    "    if sat < 1270 and sat >= 1210:\n",
    "        return 17\n",
    "    if sat < 1210 and sat >= 1140:\n",
    "        return 16\n",
    "    if sat < 1140 and sat >= 1060:\n",
    "        return 15\n",
    "    if sat < 1060 and sat >= 990:\n",
    "        return 14\n",
    "    if sat < 990:\n",
    "        return 13\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * from colleges.university_profiles as c JOIN profiles as p on c.school=p.school\",conn)\n",
    "#df = pd.read_sql_query(\"SELECT * from colleges.university_profiles as c JOIN profiles as p on c.school=p.school where c.school like '%University Park%' and act not like 'None' and gpa_w not like 'None'\",conn)\n",
    "# df  = pd.read_sql_query(\"SELECT * from profiles\", conn)\n",
    "\n",
    "# CLEAN UNIVERSITY DATA\n",
    "# drop sat averages\n",
    "for col in ['sat_math', 'sat_reading', 'sat_writing']:\n",
    "    df = df.drop(col, axis=1)\n",
    "# remove duplicate school column\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "# handle nones in all columns\n",
    "df.fillna(value=pd.np.nan, inplace=True)\n",
    "# remove - in avg_act\n",
    "df['avg_act'] = df['avg_act'].map(lambda x: str(x).strip('-'))\n",
    "# cost attendance in state vs out state\n",
    "df['cost_attendance'] = df['cost_attendance'].map(lambda x: x[x.find(\"$\")+1:x.find(\"<\")-1] if not isinstance(x,float) else x)\n",
    "# fix faculty total none as string bug\n",
    "df[\"faculty_total\"] = df[\"faculty_total\"].map(lambda x : None if x == \"None\" else x)\n",
    "# weird act nan bug\n",
    "df = df[df.avg_act != \"nan\"]\n",
    "\n",
    "# CLEAN INDIVIDUAL PROFILES\n",
    "# sums profiles sat individual scores and drops them\n",
    "df['sat_c'] = df['sat_m']+df['sat_r']+df['sat_w']\n",
    "for col in ['sat_m', 'sat_r', 'sat_w']:\n",
    "    df = df.drop(col, axis=1)\n",
    "# converts all sat to act if act is not present\n",
    "df['act'] = df.apply(lambda x: convert_sat_to_act(x['sat_c'])if pd.isnull(x['act']) else x['act'],axis=1)\n",
    "# drop nan\n",
    "df = df.dropna()\n",
    "# drop class ranks that are not in \"1 of 200\" format\n",
    "df = df[df.class_rank.str.match('\\d* (of) \\d*', na=False)]\n",
    "# converts class rank to decimal\n",
    "df['class_rank'] = df['class_rank'].apply(lambda x: convert_class_rank(x))\n",
    "# convert string label to int\n",
    "replace_columns = ['hs_type', 'gender', 'status', 'hs_state', 'school', 'institution_type', 'state', 'city']\n",
    "df['status'].replace({'Deferred': 'Denied', 'Wait-Listed': 'Denied', 'Will Attend': 'Accepted'}, inplace=True)\n",
    "for col in replace_columns:\n",
    "    df[col].replace(get_type_map(df[col]), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school                   18835\n",
      "state                    18835\n",
      "city                     18835\n",
      "avg_gpa                  18835\n",
      "avg_act                  18835\n",
      "cost_attendance          18835\n",
      "average_freshman_aid     18835\n",
      "admission_rate           18835\n",
      "faculty_total            18835\n",
      "international_percent    18835\n",
      "institution_type         18835\n",
      "female_percentage        18835\n",
      "year                     18835\n",
      "gender                   18835\n",
      "hs_type                  18835\n",
      "hs_state                 18835\n",
      "gpa_w                    18835\n",
      "gpa_uw                   18835\n",
      "act                      18835\n",
      "class_rank               18835\n",
      "status                   18835\n",
      "eaed                     18835\n",
      "legacy                   18835\n",
      "athlete                  18835\n",
      "sat_c                    18835\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('year', axis=1)\n",
    "X = X.drop('status', axis=1)\n",
    "X = X.drop('athlete', axis=1)\n",
    "X = X.drop('sat_c', axis=1)\n",
    "    \n",
    "y = df['status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       school  state  city avg_gpa avg_act cost_attendance  \\\n",
      "72549      28     45    43    3.50      23            2428   \n",
      "76889     536     14    96    4.02      32            3060   \n",
      "29805     235      3   150    3.66      27             693   \n",
      "47917     128      6   362    3.94      31             661   \n",
      "63406     405     50   220    3.50      30             662   \n",
      "83428     666     27   500    4.27      31            3150   \n",
      "7745      563     24   512    2.85      23            2404   \n",
      "7445      415     24   295    3.83      28            2547   \n",
      "77799     207     32   239    3.93      33             703   \n",
      "75408     326     50   433    3.93      28            2250   \n",
      "33302     655     44   485    4.18      34             696   \n",
      "57186     489      6   135    3.70      25            2822   \n",
      "47084     238      6   387    3.69      30             694   \n",
      "11483     319     24   253    3.63      25             654   \n",
      "34462     564     44   308    3.71      26             697   \n",
      "83795     589     27   437    3.24      19             505   \n",
      "41339     637      6   342    3.32      24             537   \n",
      "56014     114      6   471    3.68      21             598   \n",
      "90161     194     36   332    3.40      25             528   \n",
      "60        571     24    66    3.52      25             567   \n",
      "38368     479      8   176    3.80      31            3364   \n",
      "1066      435     24   496    3.50      26             604   \n",
      "18812     396     44     3    3.89      28            3025   \n",
      "40325     390      6   156    3.74      21             483   \n",
      "53352     169     32   239    3.42      25             634   \n",
      "68622     154     10    97    3.57      23            2692   \n",
      "85780      11     13   331    4.67      30            2381   \n",
      "17344     317     33   305    3.66      28            3327   \n",
      "2008      200     12   315    3.45      25             590   \n",
      "13186     531     44   308    3.66      31             703   \n",
      "...       ...    ...   ...     ...     ...             ...   \n",
      "58162     510     14    72    4.00      29            2640   \n",
      "4454      456      3   150    3.85      33             743   \n",
      "68655     154     10    97    3.57      23            2692   \n",
      "33638     655     44   485    4.18      34             696   \n",
      "47480     128      6   362    3.94      31             661   \n",
      "85908      11     13   331    4.67      30            2381   \n",
      "67790     197      6   256    4.36      29            3048   \n",
      "44491     327      8   298    3.62      25            2646   \n",
      "81464     187     28   302    3.56      30             707   \n",
      "69730     162      6   256    3.76      30             707   \n",
      "83302     666     27   500    4.27      31            3150   \n",
      "85742      11     13   331    4.67      30            2381   \n",
      "90375     532     23   470    3.84      29            2569   \n",
      "76590     536     14    96    4.02      32            3060   \n",
      "30807     545     49   426    3.05      24             565   \n",
      "27155     121     35   337    3.84      29            2952   \n",
      "9267      286     12   316    3.91      31             665   \n",
      "53250     169     32   239    3.42      25             634   \n",
      "53005     297     32   472    3.50      25             558   \n",
      "70582     471     29   416    3.62      29             747   \n",
      "8648      286     12   316    3.91      31             665   \n",
      "5044      456      3   150    3.85      33             743   \n",
      "47359     128      6   362    3.94      31             661   \n",
      "65028     507      6    57    3.87      29            3577   \n",
      "82165     631      9   278    3.80      32             654   \n",
      "77304     448     50    93    4.04      23            2223   \n",
      "88955     401     38    99    3.71      25            2779   \n",
      "47813     128      6   362    3.94      31             661   \n",
      "47927     128      6   362    3.94      31             661   \n",
      "66859     635      6     7    4.08      29            2753   \n",
      "\n",
      "      average_freshman_aid admission_rate faculty_total international_percent  \\\n",
      "72549                14375             62          1022                   1.4   \n",
      "76889                14735             23          1115                   9.0   \n",
      "29805                42948             79          1108                   11.   \n",
      "47917                53337              5          1613                   9.1   \n",
      "63406                46432             36          1115                   14.   \n",
      "83428                29009             27          1482                   4.4   \n",
      "7745                 11532             53           374                   0.9   \n",
      "7445                 14202             42          1075                   14.   \n",
      "77799                41053             12           637                   9.4   \n",
      "75408                 8544             50          1572                   1.8   \n",
      "33302                55354              5           975                   11.   \n",
      "57186                 9700             35           876                   7.1   \n",
      "47084                38585             54           542                   3.9   \n",
      "11483                38450             47          1129                   12.   \n",
      "34462                25664             46           202                   9.1   \n",
      "83795                26090             71            91                   3.1   \n",
      "41339                22083             69           274                   3.1   \n",
      "56014                37400             73           190                   3.5   \n",
      "90161                34185             72           115                   1.0   \n",
      "60                   21500             73           335                   4.0   \n",
      "38368                15356             40           285                   6.8   \n",
      "1066                 30684             68           638                   3.7   \n",
      "18812                17495             57          1351                   5.9   \n",
      "40325                25615             60           485                   2.9   \n",
      "53352                32616             66           369                   1.0   \n",
      "68622                13241             81          1132                   4.4   \n",
      "85780                17732             24          1672                   2.7   \n",
      "17344                26574             67           629                   5.1   \n",
      "2008                 15530             79           459                   2.6   \n",
      "13186                47702             25          1839                   21.   \n",
      "...                    ...            ...           ...                   ...   \n",
      "58162                13999             54          2028                   1.6   \n",
      "4454                 49401              9          1519                   12.   \n",
      "68655                13241             81          1132                   4.4   \n",
      "33638                55354              5           975                   11.   \n",
      "47480                53337              5          1613                   9.1   \n",
      "85908                17732             24          1672                   2.7   \n",
      "67790                24405             18          1570                   11.   \n",
      "44491                11572             83          1318                   3.4   \n",
      "81464                45405             21           767                   4.2   \n",
      "69730                52795             16          2133                   13.   \n",
      "83302                29009             27          1482                   4.4   \n",
      "85742                17732             24          1672                   2.7   \n",
      "90375                16644             54          2392                   8.5   \n",
      "76590                14735             23          1115                   9.0   \n",
      "30807                27225             81           355                   5.1   \n",
      "27155                27050             29          2791                   7.1   \n",
      "9267                 54310              6           959                   11.   \n",
      "53250                32616             66           369                   1.0   \n",
      "53005                39191             85           133                   3.5   \n",
      "70582                43715             49           758                   8.6   \n",
      "8648                 54310              6           959                   11.   \n",
      "5044                 49401              9          1519                   12.   \n",
      "47359                53337              5          1613                   9.1   \n",
      "65028                24668             15          1623                   13.   \n",
      "82165                50356             11           956                   7.1   \n",
      "77304                13295             60           826                   3.0   \n",
      "88955                13284             65          1198                   4.0   \n",
      "47813                53337              5          1613                   9.1   \n",
      "47927                53337              5          1613                   9.1   \n",
      "66859                24736             34          1123                   19.   \n",
      "\n",
      "        ...    female_percentage gender  hs_type  hs_state  gpa_w  gpa_uw  \\\n",
      "72549   ...                 59.5      0        1        50   4.00    3.77   \n",
      "76889   ...                 38.0      2        1        45   4.30    3.91   \n",
      "29805   ...                 48.3      0        1        12   4.24    3.69   \n",
      "47917   ...                 48.5      2        1        29   4.87    4.00   \n",
      "63406   ...                 52.3      0        0        55   3.89    3.57   \n",
      "83428   ...                 54.7      0        1        49   4.08    3.45   \n",
      "7745    ...                 56.9      0        1        26   3.60    3.55   \n",
      "7445    ...                 46.7      0        1        12   3.80    3.80   \n",
      "77799   ...                 50.6      2        0        29   4.20    4.00   \n",
      "75408   ...                 54.3      2        1        55   3.70    3.00   \n",
      "33302   ...                 47.7      0        1        49   4.54    4.00   \n",
      "57186   ...                 54.2      0        1         6   4.37    3.66   \n",
      "47084   ...                 49.8      0        1         6   4.80    3.99   \n",
      "11483   ...                 53.9      0        1         3   4.00    3.80   \n",
      "34462   ...                 59.5      0        1        49   4.29    3.75   \n",
      "83795   ...                 59.7      0        1        25   4.21    3.46   \n",
      "41339   ...                 63.7      2        1        45   4.26    3.98   \n",
      "56014   ...                 57.4      0        1        33   4.05    4.00   \n",
      "90161   ...                 53.8      0        1         6   4.75    4.00   \n",
      "60      ...                 68.4      0        1         3   4.14    3.67   \n",
      "38368   ...                 27.9      2        1         6   4.00    3.86   \n",
      "1066    ...                 57.6      0        1         6   4.05    4.00   \n",
      "18812   ...                 49.8      0        1        54   3.70    3.30   \n",
      "40325   ...                 66.1      0        1         6   3.80    3.60   \n",
      "53352   ...                 57.3      0        1        35   4.17    3.72   \n",
      "68622   ...                 60.0      2        1         6   3.75    3.51   \n",
      "85780   ...                 58.8      0        1        32   4.84    3.65   \n",
      "17344   ...                 58.0      2        1        49   3.82    3.50   \n",
      "2008    ...                 58.8      0        1        12   3.85    3.85   \n",
      "13186   ...                 60.1      0        1        26   3.80    3.70   \n",
      "...     ...                  ...    ...      ...       ...    ...     ...   \n",
      "58162   ...                 56.8      2        3        55   4.31    3.86   \n",
      "4454    ...                 51.0      0        0        40   4.60    4.00   \n",
      "68655   ...                 60.0      2        1        10   3.70    3.10   \n",
      "33638   ...                 47.7      0        1        21   4.60    4.00   \n",
      "47480   ...                 48.5      0        1        32   4.93    3.98   \n",
      "85908   ...                 58.8      2        1        25   4.10    3.95   \n",
      "67790   ...                 57.5      0        1         6   4.30    4.00   \n",
      "44491   ...                 51.9      2        1         8   3.83    3.56   \n",
      "81464   ...                 59.1      0        1         6   3.52    3.48   \n",
      "69730   ...                 51.5      0        1         6   3.81    3.69   \n",
      "83302   ...                 54.7      0        1        35   4.33    3.91   \n",
      "85742   ...                 58.8      2        1         3   4.26    3.95   \n",
      "90375   ...                 51.2      0        1        40   4.33    3.90   \n",
      "76590   ...                 38.0      2        1        29   3.40    3.10   \n",
      "30807   ...                 51.1      0        1        26   3.70    3.70   \n",
      "27155   ...                 49.8      2        1        42   4.19    3.99   \n",
      "9267    ...                 48.6      2        1        46   4.37    3.85   \n",
      "53250   ...                 57.3      0        1        26   4.00    4.00   \n",
      "53005   ...                 50.9      2        1        25   4.30    4.00   \n",
      "70582   ...                 49.5      2        3        32   4.00    3.86   \n",
      "8648    ...                 48.6      0        1        55   4.80    3.95   \n",
      "5044    ...                 51.0      2        1        54   4.92    4.00   \n",
      "47359   ...                 48.5      0        0         6   4.50    3.90   \n",
      "65028   ...                 52.1      0        1        19   4.63    3.89   \n",
      "82165   ...                 50.5      2        1        21   4.50    3.90   \n",
      "77304   ...                 55.9      2        1        55   4.40    3.16   \n",
      "88955   ...                 57.6      0        0         3   3.96    3.96   \n",
      "47813   ...                 48.5      0        1         1   4.80    4.00   \n",
      "47927   ...                 48.5      0        3         1   4.34    4.00   \n",
      "66859   ...                 49.3      0        1         6   4.16    3.92   \n",
      "\n",
      "        act  class_rank  eaed  legacy  \n",
      "72549  20.0    0.116608     0       0  \n",
      "76889  35.0    0.040000     0       0  \n",
      "29805  29.0    0.060784     1       0  \n",
      "47917  35.0    0.003125     0       0  \n",
      "63406  22.0    0.348837     0       0  \n",
      "83428  23.0    0.059091     0       0  \n",
      "7745   24.0    0.289256     0       0  \n",
      "7445   27.0    0.147239     0       0  \n",
      "77799  35.0    0.007692     0       0  \n",
      "75408  23.0    0.279783     0       0  \n",
      "33302  35.0    0.022222     0       0  \n",
      "57186  28.0    0.095238     0       0  \n",
      "47084  32.0    0.008427     0       0  \n",
      "11483  32.0    0.063559     0       0  \n",
      "34462  28.0    0.030769     1       0  \n",
      "83795  25.0    0.272472     1       0  \n",
      "41339  32.0    0.055288     0       0  \n",
      "56014  26.0    0.007313     0       0  \n",
      "90161  31.0    0.006667     0       0  \n",
      "60     27.0    0.084956     0       0  \n",
      "38368  31.0    0.155396     0       0  \n",
      "1066   24.0    0.055866     0       0  \n",
      "18812  29.0    0.063415     0       0  \n",
      "40325  25.0    0.097059     0       0  \n",
      "53352  26.0    0.090395     1       0  \n",
      "68622  24.0    0.177866     0       0  \n",
      "85780  31.0    0.107595     1       0  \n",
      "17344  28.0    0.145833     1       0  \n",
      "2008   24.0    0.107143     0       0  \n",
      "13186  28.0    0.096250     0       0  \n",
      "...     ...         ...   ...     ...  \n",
      "58162  30.0    0.141176     1       0  \n",
      "4454   34.0    0.006667     0       0  \n",
      "68655  19.0    0.277778     0       0  \n",
      "33638  32.0    0.002353     0       0  \n",
      "47480  35.0    0.051756     0       1  \n",
      "85908  25.0    0.003401     0       0  \n",
      "67790  36.0    0.010309     0       0  \n",
      "44491  31.0    0.170068     0       0  \n",
      "81464  22.0    0.300412     0       0  \n",
      "69730  22.0    0.136778     0       0  \n",
      "83302  32.0    0.036782     0       0  \n",
      "85742  31.0    0.045872     1       0  \n",
      "90375  33.0    0.012000     0       0  \n",
      "76590  29.0    0.467153     0       0  \n",
      "30807  20.0    0.131115     0       0  \n",
      "27155  35.0    0.004274     0       0  \n",
      "9267   34.0    0.122024     0       0  \n",
      "53250  27.0    0.060241     1       0  \n",
      "53005  34.0    0.011494     0       1  \n",
      "70582  32.0    0.034146     1       0  \n",
      "8648   32.0    0.019324     0       0  \n",
      "5044   35.0    0.003676     0       0  \n",
      "47359  30.0    0.039801     0       0  \n",
      "65028  35.0    0.011594     0       0  \n",
      "82165  28.0    0.089835     0       0  \n",
      "77304  19.0    0.537915     0       0  \n",
      "88955  25.0    0.025641     0       0  \n",
      "47813  31.0    0.001667     1       0  \n",
      "47927  34.0    0.016393     1       0  \n",
      "66859  30.0    0.086777     0       0  \n",
      "\n",
      "[4709 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/marclane/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/marclane/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "print(X_test)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(8,8,8),max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(8, 8, 8), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3385  283]\n",
      " [ 417  624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91      3668\n",
      "           1       0.69      0.60      0.64      1041\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      4709\n",
      "   macro avg       0.79      0.76      0.77      4709\n",
      "weighted avg       0.85      0.85      0.85      4709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8381822042896581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fc04aa0f438>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "statuses = cursor.execute(\"SELECT status, act, gpa_w from colleges.university_profiles as c JOIN profiles as p on c.school=p.school where c.school like '%University Park%' and status not like 'will%' and act not like 'None' and gpa_w not like 'None'\").fetchall()\n",
    "colors = {\"Accepted\":\"g\", \"Denied\":\"r\", \"Wait-Listed\":\"y\", \"Deferred\":\"orange\"}\n",
    "x = [t[1] for t in statuses]\n",
    "y = [t[2] for t in statuses]\n",
    "c = [colors[t[0]] for t in statuses]\n",
    "plt.ylabel(\"Weighted GPA\")\n",
    "plt.xlabel(\"ACT Score\")\n",
    "plt.title(\"Raw Penn State Acceptance vs. Rejection vs. Wait-Listed\")\n",
    "plt.scatter(x, y, c=c)\n",
    "\n",
    "# statuses = cursor.execute(\"SELECT status, sat_m+sat_r+sat_w, gpa_w from colleges.university_profiles as c JOIN profiles as p on c.school=p.school where c.school like '%University Park%' and status not like 'will%' and act not like 'None' and gpa_w not like 'None'\").fetchall()\n",
    "# colors = {0:\"g\", 1:\"r\"}\n",
    "# x = df['act']\n",
    "# y = df['gpa_w']\n",
    "# c = [colors[b] for b in df['status']]\n",
    "# plt.ylabel(\"Weighted GPA\")\n",
    "# plt.xlabel(\"ACT Score\")\n",
    "# plt.title(\"Predicted Penn State Acceptance vs. Rejection\")\n",
    "# plt.scatter(x, y, c=c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
